{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bd12ed0-700b-4bf4-a72f-d58a226addd7",
   "metadata": {},
   "source": [
    "# Training DNN\n",
    "- tricky vanishing gradients problem or related exploding gradients problem\n",
    "- Don't have enough training data for such a large network, or too costly to label\n",
    "- Training may be extremely slow\n",
    "- A model ith million of parameters that risk to overfitting the training set, especially if there are not enough training instances or if they are too noisy.\n",
    "\n",
    "## 1. Vanishing/Exploding Gradients Problems\n",
    "Gradients often get smaller and smaller as the algorithm progresses down to the lower layers (vanishing), or gradients can grow bigger and bigger until layers get insanely large weight updates and the algorithm diverges (Exploding).\n",
    "\n",
    "```\n",
    "+---------------+-------------------------------+---------------------+\n",
    "|Initialization |  Activation function          | $\\sigma^2$ (Normal) |\n",
    "+---------------+-------------------------------+---------------------+\n",
    "| Glorot        | None, tanh, logistic, softmax | $1/fan_{avg}$       |\n",
    "| He            | ReLU, variants                | $2/fan_{in}$        |\n",
    "| LeCun         | SELU                          | $1/fan_{in}$        |\n",
    "+---------------+-------------------------------+---------------------+\n",
    "```\n",
    "Keras uses Glorot initialization (default) with a uniform distribution. When creating a layer, we can change it to He initialization by kernel_initializer=\"he_uniform\" or \"he_normal\".\n",
    "\n",
    "If you want He initialization with a uniform distribution but based on fan avg rather than fan in , you can use the VarianceScaling initializer like this:\n",
    "``` \n",
    "he_avg_init = keras.initializers.VarianceScaling(scale=2., mode='fan_avg', distribution='uniform')\n",
    "keras.layers.Dense(10, activation=\"sigmoid\", kernel_initializer=he_avg_init)\n",
    "```\n",
    "He initialization: ``` keras.layers.Dense(10, activation=\"relu\", kernel_initializer=\"he_normal\")```\n",
    "\n",
    "If you want He initialization with a uniform distribution but based on fan avg rather than fan in , you can use the VarianceScaling initializer like this:\n",
    "```\n",
    "he_avg_init = keras.initializers.VarianceScaling(scale=2., mode='fan_avg', distribution='uniform')\n",
    "keras.layers.Dense(10, activation=\"sigmoid\", kernel_initializer=he_avg_init)\n",
    "```\n",
    "\n",
    "## 1.1. Nonsaturating Activation Functions\n",
    "The dying ReLUs: during training, some neurons effectively \"die\", meaning they stop outputting anything other than 0. \n",
    "- Solve this problem, the leaky ReLU: $leakyReLU_{\\alpha}(z) = max(\\alpha z, z)$. The hyperparameter $\\alpha$ defines how much function leak. \n",
    "- ELU (Exponential Linear Unit): like a ReLU function but, takes negative when z < 0, it has nonzero gradient for z < 0 (avoid dead neuron problem). But ELU is slower to compute than ReLU.\n",
    "- SELU (Scaled ELU): the network will self-normalize (mean=0, std=1). Using constrains: input features must be standardized, hidden layer's weight must be initialized with LeCun normal initialization (kernel_initializer=\"lecun_normal\"), the network's architecture must be Sequential(). \n",
    "\n",
    "Which activation function should use? in general: SELU > ELU > leaky ReLU (and its variants) > ReLU > tanh > logistic.\n",
    "\n",
    "Using SELU: ``` keras.layers.Dense(10, activation=\"selu\", kernel_initializer=\"lecun_normal\") ```\n",
    "\n",
    "Implementing ELU in TensorFlow is trivial, just specify the activation function when building each layer: ```keras.layers.Dense(10, activation=\"elu\") ```\n",
    "\n",
    "Sample of training model with *Leaky ReLU* activation function:\n",
    "```\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.Dense(100, kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "```\n",
    "\n",
    "## 1.2. Batch Normalization (BN)\n",
    "The technique consists of adding an operation in the model just before or after activation function of each hidden layer. This operation is simply zero-center and normalization each input, then scales and shifts the result using two new parameter vectors per layer: one for scaling, the other for shifting. In other words, the operation lets the model learn the optimal scale and mean of each of the layer's input.\n",
    "\n",
    "Implement of BN to create the model:\n",
    "```\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "```\n",
    "Let’s look at the parameters of the first BN layer. Two are trainable (by backpropagation), and two are not:\n",
    "```\n",
    ">>> [(var.name, var.trainable) for var in model.layers[1].variables]\n",
    "        [('batch_normalization_v2/gamma:0', True),\n",
    "        ('batch_normalization_v2/beta:0', True),\n",
    "        ('batch_normalization_v2/moving_mean:0', False),\n",
    "        ('batch_normalization_v2/moving_variance:0', False)]\n",
    "```\n",
    " **Notice:**\n",
    "\n",
    "> Adding BN layer before the activation functions, rather than after. To add the BN function before the activation functions, we must remove the activation function from the hidden layers and add them as separate layer after BN layer. Moreover, the BN layer includes on offset parameter per input, we can remove the bias term from the previous hidden layer by using use_bias=False. Sample code:\n",
    "```\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, kernel_initializer=\"he_normal\", use_bias=False),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation(\"elu\"),\n",
    "    keras.layers.Dense(100, kernel_initializer=\"he_normal\", use_bias=False),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation(\"elu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "```\n",
    "\n",
    "## 1.3. Gradient Clipping\n",
    "Mitigate the exploding gradients problem is to clip the gradients during backpropagation so that they never exceed some threshold. \n",
    "\n",
    "This technique is most often used in the recurrent neural networks, BN is tricky to use in RNN.\n",
    "\n",
    "In Keras, implementing Gradient Clipping is just a matter of setting the clipvalue or clipnorm argument when creating an optimizer, like this:\n",
    "```\n",
    "optimizer = keras.optimizers.SGD(clipvalue=1.0)\n",
    "model.compile(loss=\"mse\", optimizer=optimizer)\n",
    "```\n",
    "This optimizer will clip every component of the gradient vector to a value between –1.0 and 1.0. This means that all the partial derivatives of the loss (with regard to each and every trainable parameter) will be clipped between –1.0 and 1.0. The threshold is a hyperparameter you can tune. Note that it may change the orientation of the gradient vector. Using *clipnorm* instead of *clipvalue* for not change the direction of the gradient vector.\n",
    "\n",
    "## 2. Reusing Pretrained layers\n",
    "Reusing a existing neural network that accomplishes a similar task, and then reuse the lower layers of this network. This is called *transfer learning*.\n",
    "\n",
    "> If the input pictures of your new task don’t have the same size as the ones used in the original task, you will usually have to add a preprocessing step to resize them to the size expected by the original model. More generally, transfer learning will work best when the inputs have similar low-level features.\n",
    "\n",
    "The output layer of the original model should usually be replaced because it is most likely not useful at all for the new task, and it may not even have the right number of outputs for the new task.\n",
    "\n",
    "First, you need to load model A and create a new model based on that model’s layers. Let’s reuse all the layers except for the output layer:\n",
    "```\n",
    "model_A = keras.models.load_model(\"my_model_A.h5\")\n",
    "model_B_on_A = keras.models.Sequential(model_A.layers[:-1])\n",
    "model_B_on_A.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "```\n",
    "Note that model_A and model_B_on_A now share some layers. When you train model_B_on_A , it will also affect model_A . If you want to avoid that, you need to clone model_A before you reuse its layers. To do this, you clone model A’s architecture with clone_model() , then copy its weights (since clone_model() does not clone the weights):\n",
    "```\n",
    "model_A_clone = keras.models.clone_model(model_A)\n",
    "model_A_clone.set_weights(model_A.get_weights())\n",
    "```\n",
    "## 2.1. Unsupervised Pretraining\n",
    "This technique is called *greedy layer-wise pretraining*. They would first train an unsupervised model with a single layer, typically an RBM, then they would freeze that layer and add another one on top of it, then train the model again (effectively just training the new layer), then freeze the new layer and add another layer on top of it, train the model again, and so on. Nowadays, things are much simpler: people generally train the full unsupervised model in one shot (i.e., in Figure 11-5, just start directly at step three) and use autoencoders or GANs rather than RBMs.\n",
    "\n",
    "<img src=\"images/deeplearningUnlabeledmodel.png\" width=\"442\" height=\"442\" style=\"vertical-align:middle;\"/>\n",
    "\n",
    "*Figure 11-5. In unsupervised training, a model is trained on the unlabeled data (or on all the data) using an unsupervised learning technique, then it is fine-tuned for the finaltask on the labeled data using a supervised learning technique; the unsupervised part may train one layer at a time as shown here, or it may train the full model directly*\n",
    "\n",
    "## 2.2. Pretraining on an Auxiliary task\n",
    "One last option is to train a first neural network on an auxiliary task for which you can easily obtain or generate labeled training data, then reuse the lower layers of that network for your actual task. The first neural network’s lower layers will learn feature detectors that will likely be reusable by the second neural network.\n",
    "\n",
    "Self-supervised learning is when you automatically generate the labels from the data itself, then you train a model on the resulting“labeled” dataset using supervised learning techniques. Since this approach requires no human labeling whatsoever, it is best classified as a form of unsupervised learning.\n",
    "\n",
    "## 3. Faster Optimizers\n",
    "Speed up training very large NN with some techniques:\n",
    "- Applying a good initialization strategy for the connection weights.\n",
    "- Using a good activation function\n",
    "- Using Batch Normalization\n",
    "- Reusing parts of a pretrained network (possibly build on an auxiliary task or using unsupervised learning)\n",
    "\n",
    "Other techniques:\n",
    "- momentum optimization\n",
    "- Nesterov accelerated gradient\n",
    "- AdaGrad\n",
    "- RMSProp\n",
    "- Adam and Nadam optimization\n",
    "\n",
    "## 3.1. Momentum Optimization\n",
    "GD takes small steps down the slope, so the algorithm will take much more time to reach the bottom. Momentum opt cares about what previous gradients were: at each iteration, it subtracts the local gradient from the momentum vector m, and update the weight by adding this momentum vector. Value Range: 0 (high friction) to 1 (no friction). *A typical momentum value is 0.9*\n",
    "```\n",
    "optimizer = keras.optimizers.SGD(learning_rate=0.001, momentum=0.9)\n",
    "```\n",
    "## 3.2. Nesterov Accelerated Gradient\n",
    "Measure the gradient of the cost function not at the local position $ \\theta $ but slightly ahead in the direction of the momentum, at $ \\theta + \\beta m$\n",
    "\n",
    "NAG is faster than regular momentum optimization.\n",
    "```\n",
    "optimizer = keras.optimizers.SGD(learning_rate=0.001, momentum=0.9, nesterov=True) \n",
    "```\n",
    "## 3.3. AdaGrad\n",
    "The AdaGrad algorithm achieves this correction by scaling down the gradient vector along the steepest dimensions. AdaGrad frequently performs well for simple quadratic problems, but it often stops too early when training neural networks. The learning rate gets scaled down so much that the algorithm ends up stopping entirely before reaching the global optimum. So even though Keras has an Adagrad optimizer, you should not use it to train deep neural networks (it may be efficient for simpler tasks such as Linear Regression, though). Still, understanding AdaGrad is helpful to grasp the other adaptive learning rate optimizers.\n",
    "```\n",
    "optimizer = keras.optimizers.Adagrad(learning_rate=0.001)\n",
    "```\n",
    "## 3.4. RMSProp\n",
    "Better than AdaGrad and fixes its problems.\n",
    "```\n",
    "optimizer = keras.optimizers.RMSprop(learning_rate=0.001, rho=0.9)\n",
    "```\n",
    "## 3.5. Adam Optimization\n",
    "Adam (adaptive moment estimation), Since Adam is an adaptive learning rate algorithm (like AdaGrad and RMSProp), it requires less tuning of the learning rate hyperparameter η. You can often use the default value η = 0.001, making Adam even easier to use than Gradient Descent. \n",
    "```\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)\n",
    "```\n",
    "## 3.6. Adamax Optimization\n",
    "AdaMax, introduced in the same paper as Adam, replaces the ℓ_2 norm with the ℓ_∞ norm (a fancy way of saying the max)\n",
    "```\n",
    "optimizer = keras.optimizers.Adamax(learning_rate=0.001, beta_1=0.9, beta_2=0.999)\n",
    "```\n",
    "## 3.7. Nadam Optimization\n",
    "Nadam optimization is Adam optimization plus the Nesterov trick, so it will often converge slightly faster than Adam.\n",
    "```\n",
    "optimizer = keras.optimizers.Nadam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)\n",
    "```\n",
    "## 4. Training Sparse models\n",
    "All the optimization algorithms  just presented produce dense model (most parameters will be nonzero). Sparse models are faster at runtime, less memory using (most parameters are zero). To use Sparse models, applying l_1 or l_2 regularization (in Lasso Regression)\n",
    "\n",
    "## 5. Learning Rate Scheduling\n",
    "Find a good learning rate. There are some strategies are called learning schedules:\n",
    "- Power scheduling: learning rate drops at each step\n",
    "- Exponential scheduling: the learning rate will gradually drop by a factor of 10 every s steps.\n",
    "- Piecewise constant scheduling: using a constant learning rate for number of epochs (eg., learning_rate=0.1 for 5 epochs)\n",
    "- Performance scheduling: measure the validation error every N steps\n",
    "- 1cycle scheduling: 1cycle starts by increasing the initial learning rate η_0 , growing linearly up to η_1 halfway through training. Then it decreases the learning rate linearly down to η_0 again during the second half of training, finishing the last few epochs by dropping the rate down by several orders of magnitude (still linearly). The maximum learning rate η_1 is chosen using the same approach we used to find the optimal learning rate, and the initial learning rate η_0 is chosen to be roughly 10 times lower.\n",
    "\n",
    "Implementing power scheduling in Keras by setting the decay hyperparameter when creating an optimizer:\n",
    "```\n",
    "optimizer = keras.optimizers.SGD(lr=0.01, decay=1e-4)\n",
    "```\n",
    "The *decay* is the inverse of s (number of steps) and Keras assume that c is equal to 1.\n",
    "\n",
    "> Customize learning rate scheduling: https://github.com/ageron/handson-ml2/blob/master/11_training_deep_neural_networks.ipynb\n",
    "\n",
    "## 6. Avoiding Overfitting through Regularization\n",
    "- Early stopping technique is one of the best regularization, Batch Normalization solve the unstable gradients problem.\n",
    "### 6.1. l_1 and l_2 Regularization\n",
    "We can use ℓ 2 regularization to constrain a neural network’s connection weights, and/or ℓ 1 regularization if you want a sparse model (with many weights equal to 0). Here is how to apply ℓ 2 regularization to a Keras layer’s connection weights, using a regularization factor of 0.01:\n",
    "```\n",
    "layer = keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\",\n",
    "                           kernel_regularizer=keras.regularizers.l2(0.01))\n",
    "```\n",
    "\n",
    "Customize regularization with Python's functools.partial() function, which is quickly creating a thin wrapper for any callable:\n",
    "```\n",
    "from functools import partial\n",
    "\n",
    "RegularizedDense = partial(keras.layers.Dense,\n",
    "        activation=\"elu\",\n",
    "        kernel_initializer=\"he_normal\",\n",
    "        kernel_regularizer=keras.regularizers.l2(0.01))\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    RegularizedDense(300),\n",
    "    RegularizedDense(100),\n",
    "    RegularizedDense(10, activation=\"softmax\",\n",
    "    kernel_initializer=\"glorot_uniform\")\n",
    "])\n",
    "```\n",
    "\n",
    "### 6.2. Dropout\n",
    "The most popular regularization techniques for deep neural networks. It is a fairly simple algorithm: at every training step, every neuron (including the input neurons, but always excluding the output neurons) has a probability p of being temporarily “dropped out,” meaning it will be entirely ignored during this training step, but it may be active during the next step (see Figure 11-9). The hyperparameter p is called the dropout rate, and it is typically set between 10% and 50%: closer to 20–30% in recurrent neural nets (see Chapter 15), and closer to 40–50% in convolutional neural networks (see Chapter 14). After training, neurons don’t get dropped anymore. And that’s all (except for a technical detail we will discuss momentarily).\n",
    "\n",
    "The following code applies dropout regularization before every Dense layer, using a dropout rate of 0.2:\n",
    "```\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "```\n",
    "\n",
    "> Since dropout is only active during training, comparing the training loss and the validation loss can be misleading. In particular, a model may be overfitting the training set and yet have similar training and validation losses. So make sure to evaluate the training loss without dropout (e.g., after training).\n",
    "\n",
    "### 6.3. Monte Carlo (MC) Dropout\n",
    "(Hand-on ML with scikit-learn, keras, and tensorflow, page 368)\n",
    "### 6.4. Max-Norm Regularization\n",
    "\n",
    "for each neuron, it constrains the weights w of the incoming connections such that ∥ w ∥_2 ≤ r, where r is the max-norm hyperparameter and ∥ · ∥_2 is the ℓ_2 norm.\n",
    "\n",
    "To implement max-norm regularization in Keras, set the kernel_constraint argument of each hidden layer to a max_norm() constraint with the appropriate max value, like this:\n",
    "```\n",
    "keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\", kernel_constraint=keras.constraints.max_norm(1.))\n",
    "```\n",
    "\n",
    "# Summary \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3648db8-20cd-4c06-b1a7-d7a127a48d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d641db2-8a96-4ec1-8666-45ba701f20f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Constant',\n",
       " 'GlorotNormal',\n",
       " 'GlorotUniform',\n",
       " 'HeNormal',\n",
       " 'HeUniform',\n",
       " 'Identity',\n",
       " 'Initializer',\n",
       " 'LecunNormal',\n",
       " 'LecunUniform',\n",
       " 'Ones',\n",
       " 'Orthogonal',\n",
       " 'RandomNormal',\n",
       " 'RandomUniform',\n",
       " 'TruncatedNormal',\n",
       " 'VarianceScaling',\n",
       " 'Zeros',\n",
       " 'constant',\n",
       " 'deserialize',\n",
       " 'get',\n",
       " 'glorot_normal',\n",
       " 'glorot_uniform',\n",
       " 'he_normal',\n",
       " 'he_uniform',\n",
       " 'identity',\n",
       " 'lecun_normal',\n",
       " 'lecun_uniform',\n",
       " 'ones',\n",
       " 'orthogonal',\n",
       " 'random_normal',\n",
       " 'random_uniform',\n",
       " 'serialize',\n",
       " 'truncated_normal',\n",
       " 'variance_scaling',\n",
       " 'zeros']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show all available initializers\n",
    "[name for name in dir(keras.initializers) if not name.startswith('_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a225c631-3e6d-49a4-b224-2448550be4c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['deserialize',\n",
       " 'elu',\n",
       " 'exponential',\n",
       " 'gelu',\n",
       " 'get',\n",
       " 'hard_sigmoid',\n",
       " 'linear',\n",
       " 'relu',\n",
       " 'selu',\n",
       " 'serialize',\n",
       " 'sigmoid',\n",
       " 'softmax',\n",
       " 'softplus',\n",
       " 'softsign',\n",
       " 'swish',\n",
       " 'tanh']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[m for m in dir(keras.activations) if not m.startswith(\"_\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4f2c626-1b74-4a01-a03a-5889a1306806",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LeakyReLU', 'PReLU', 'ReLU', 'ThresholdedReLU']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[m for m in dir(keras.layers) if \"relu\" in m.lower()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365a7401-0b74-4606-8acd-51f2046f03d3",
   "metadata": {},
   "source": [
    "# Reusing Pretrained layers\n",
    "Let's split the fashion MNIST training set in two:\n",
    "\n",
    "- X_train_A: all images of all items except for sandals and shirts (classes 5 and 6).\n",
    "- X_train_B: a much smaller training set of just the first 200 images of sandals or shirts.\n",
    "\n",
    "The validation set and the test set are also split this way, but without restricting the number of images.\n",
    "\n",
    "We will train a model on set A (classification task with 8 classes), and try to reuse it to tackle set B (binary classification). We hope to transfer a little bit of knowledge from task A to task B, since classes in set A (sneakers, ankle boots, coats, t-shirts, etc.) are somewhat similar to classes in set B (sandals and shirts). However, since we are using Dense layers, only patterns that occur at the same location can be reused (in contrast, convolutional layers will transfer much better, since learned patterns can be detected anywhere on the image, as we will see in the CNN chapter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e24e786-2917-47d5-9995-08f945cd2ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data set\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "X_train_full = X_train_full / 255.0\n",
    "X_test = X_test / 255.0\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d30c24b-9b21-46ef-8c29-bc8846ae26a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(X, y):\n",
    "    y_5_or_6 = (y == 5) | (y == 6) # sandals or shirts\n",
    "    y_A = y[~y_5_or_6]\n",
    "    y_A[y_A > 6] -= 2 # class indices 7, 8, 9 should be moved to 5, 6, 7\n",
    "    y_B = (y[y_5_or_6] == 6).astype(np.float32) # binary classification task: is it a shirt (class 6)?\n",
    "    return ((X[~y_5_or_6], y_A),\n",
    "            (X[y_5_or_6], y_B))\n",
    "\n",
    "(X_train_A, y_train_A), (X_train_B, y_train_B) = split_dataset(X_train, y_train)\n",
    "(X_valid_A, y_valid_A), (X_valid_B, y_valid_B) = split_dataset(X_valid, y_valid)\n",
    "(X_test_A, y_test_A), (X_test_B, y_test_B) = split_dataset(X_test, y_test)\n",
    "X_train_B = X_train_B[:200]\n",
    "y_train_B = y_train_B[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "997f8221-6936-4556-bfc8-bed8b863bfdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-10 23:34:56.118559: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-10 23:34:56.183095: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-10 23:34:56.183267: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 300)               235500    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               30100     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 50)                5050      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 8)                 408       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 276,158\n",
      "Trainable params: 276,158\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-10 23:34:56.184856: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-04-10 23:34:56.186543: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-10 23:34:56.186702: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-10 23:34:56.186820: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-10 23:34:56.475722: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-10 23:34:56.475846: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-10 23:34:56.475923: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-10 23:34:56.476006: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1702 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "# define the model A\n",
    "model_A = keras.models.Sequential()\n",
    "model_A.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "for n_hidden in (300, 100, 50, 50, 50):\n",
    "    model_A.add(keras.layers.Dense(n_hidden, activation=\"selu\"))\n",
    "model_A.add(keras.layers.Dense(8, activation=\"softmax\"))\n",
    "\n",
    "model_A.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "model_A.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e269ee1-507b-4f05-ab30-d5daf37d07ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-10 23:36:22.712938: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    }
   ],
   "source": [
    "history = model_A.fit(X_train_A, y_train_A, epochs=20,\n",
    "                    validation_data=(X_valid_A, y_valid_A), verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fd7f546-dbb9-456a-9bf8-131d82e9c5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A.save('models/my_model_A.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f2355ae-103f-4085-9928-c3636ec40569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_1 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 300)               235500    \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 100)               30100     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 50)                5050      \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 275,801\n",
      "Trainable params: 275,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# define model B\n",
    "model_B = keras.models.Sequential()\n",
    "model_B.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "for n_hidden in (300, 100, 50, 50, 50):\n",
    "    model_B.add(keras.layers.Dense(n_hidden, activation=\"selu\"))\n",
    "model_B.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model_B.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "model_B.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7abf7f4-bd35-49ff-ab14-1f4b1872323d",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_B.fit(X_train_B, y_train_B, epochs=20,\n",
    "                      validation_data=(X_valid_B, y_valid_B), verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3659ab62-8db2-4acc-9e8c-bebe30ede68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model A\n",
    "model_A = keras.models.load_model('models/my_model_A.h5')\n",
    "# create new clone of model A and reuse lower layers of A to B\n",
    "# when training B on A and A is not update \n",
    "model_A_clone = keras.models.clone_model(model_A)\n",
    "model_A_clone.set_weights(model_A.get_weights())  # reset the weights of clone_A by A\n",
    "model_B_on_A = keras.models.Sequential(model_A_clone.layers[:-1])\n",
    "model_B_on_A.add(keras.layers.Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5dcc94d6-5d99-42f8-9d74-12bb94c1413d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9109138250350952, 0.841697096824646, 0.7791298031806946, 0.7228350043296814]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# because model_B_on_A for task B, that will make some large error when training with few epochs (output layer was initialized randomly).\n",
    "# We train it with few epochs which setting attribute trainable=False\n",
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model_B_on_A.compile(loss=\"binary_crossentropy\",\n",
    "                     optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "                     metrics=[\"accuracy\"])\n",
    "\n",
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=4,\n",
    "                           validation_data=(X_valid_B, y_valid_B), verbose=0)\n",
    "\n",
    "history.history['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16cda794-36c6-4c1b-a338-69c39d430857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.5641 - accuracy: 0.7250 - val_loss: 0.4369 - val_accuracy: 0.8235\n",
      "Epoch 2/16\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.3583 - accuracy: 0.8800 - val_loss: 0.3229 - val_accuracy: 0.8935\n",
      "Epoch 3/16\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.2639 - accuracy: 0.9350 - val_loss: 0.2589 - val_accuracy: 0.9280\n",
      "Epoch 4/16\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.2094 - accuracy: 0.9600 - val_loss: 0.2195 - val_accuracy: 0.9462\n",
      "Epoch 5/16\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.1745 - accuracy: 0.9650 - val_loss: 0.1903 - val_accuracy: 0.9544\n",
      "Epoch 6/16\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.1491 - accuracy: 0.9750 - val_loss: 0.1700 - val_accuracy: 0.9594\n",
      "Epoch 7/16\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.1310 - accuracy: 0.9750 - val_loss: 0.1538 - val_accuracy: 0.9645\n",
      "Epoch 8/16\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.1163 - accuracy: 0.9850 - val_loss: 0.1401 - val_accuracy: 0.9696\n",
      "Epoch 9/16\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.1041 - accuracy: 0.9850 - val_loss: 0.1297 - val_accuracy: 0.9716\n",
      "Epoch 10/16\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.0950 - accuracy: 0.9850 - val_loss: 0.1217 - val_accuracy: 0.9736\n",
      "Epoch 11/16\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.0877 - accuracy: 0.9850 - val_loss: 0.1149 - val_accuracy: 0.9736\n",
      "Epoch 12/16\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.0815 - accuracy: 0.9850 - val_loss: 0.1089 - val_accuracy: 0.9746\n",
      "Epoch 13/16\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.0762 - accuracy: 0.9850 - val_loss: 0.1041 - val_accuracy: 0.9746\n",
      "Epoch 14/16\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.0719 - accuracy: 0.9850 - val_loss: 0.0995 - val_accuracy: 0.9777\n",
      "Epoch 15/16\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.0677 - accuracy: 0.9850 - val_loss: 0.0951 - val_accuracy: 0.9797\n",
      "Epoch 16/16\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.0638 - accuracy: 0.9900 - val_loss: 0.0916 - val_accuracy: 0.9797\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.06381265074014664"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now, setting back the trainable=True and train it again\n",
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = True\n",
    "\n",
    "model_B_on_A.compile(loss=\"binary_crossentropy\",\n",
    "                     optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "                     metrics=[\"accuracy\"])\n",
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=16,\n",
    "                           validation_data=(X_valid_B, y_valid_B))\n",
    "\n",
    "history.history['loss'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1534d92-6ddc-4c1d-8a38-27a552323166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 838us/step - loss: 0.1015 - accuracy: 0.9795\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.10145364701747894, 0.9794999957084656]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_B.evaluate(X_test_B, y_test_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45a15c8c-87db-464f-b897-672d12d31cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 856us/step - loss: 0.0829 - accuracy: 0.9830\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.08294575661420822, 0.9829999804496765]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_B_on_A.evaluate(X_test_B, y_test_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3bec6696-d1d7-4941-9159-2a1122dadb54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2058823529411733"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the score between 2 models\n",
    "(1 - 0.9795) / (1 - 0.983)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5b61fc-ec6e-4a9d-8de4-b8e66a9e2cfd",
   "metadata": {},
   "source": [
    "# exercise\n",
    "Deep learning on CIFAR10, The dataset is composed of 60,000 32 × 32–pixel color images (50,000 for training, 10,000 for testing) with 10 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b9d4fd3-cb6a-4c2c-8fbc-bdee8903095a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import os\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb09d843-3c3c-4b65-bb01-04b15bde9c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a. Build 20 hidden layers of 100 neurons each, use He initialization ELU activation\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'))\n",
    "    \n",
    "# b. the output for 10 classes with 10 neurons, softmax activation\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "## Remember to search for the right learning rate each time you change the model's architecture or hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53765dad-53f8-495a-a7cc-448205fcdcf4",
   "metadata": {},
   "source": [
    "Let's use a Nadam optimizer with a learning rate of 5e-5. I tried learning rates 1e-5, 3e-5, 1e-4, 3e-4, 1e-3, 3e-3 and 1e-2, and I compared their learning curves for 10 epochs each (using the TensorBoard callback, below). The learning rates 3e-5 and 1e-4 were pretty good, so I tried 5e-5, which turned out to be slightly better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "162e385d-512a-46e4-a61b-23fc6824c1d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_1 (Flatten)         (None, 3072)              0         \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 100)               307300    \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_30 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_31 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_32 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_33 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_34 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_35 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_36 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_37 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_38 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_39 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_40 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_41 (Dense)            (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 500,210\n",
      "Trainable params: 500,210\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "optimizer = keras.optimizers.Nadam(learning_rate=5e-5)\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4f65bd1-ec06-4d5c-a79a-77d39975909c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 51s 0us/step\n",
      "170508288/170498071 [==============================] - 51s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# load CIFAR10 dataset\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "X_train = X_train_full[5000:]\n",
    "y_train = y_train_full[5000:]\n",
    "X_valid = X_train_full[:5000]\n",
    "y_valid = y_train_full[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "988a303d-d5cf-4877-9ae8-21be1c00365d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we can create the callbacks and train the model\n",
    "early_stop_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"models/my_cifar10_checkpoint.h5\", save_best_only=True)\n",
    "run_index = 1   # increase when training \n",
    "run_logdir = os.path.join(os.curdir, \"my_cifar10_logs\", \"run_{:03d}\".format(run_index))\n",
    "\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "callbacks = [early_stop_cb, model_checkpoint_cb, tensorboard_cb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10293981-b5ed-4f37-b398-112349cc3670",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-11 21:40:55.948048: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fcb64119c70>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=100, validation_data=(X_valid, y_valid), callbacks=callbacks, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "367d818e-ee17-4bb5-93d8-96b246e9ef96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 0s 1ms/step - loss: 1.5629 - accuracy: 0.4750\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.562867522239685, 0.4749999940395355]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b19c078-dad9-4ce3-ace5-5578bcfb416b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 0s 1ms/step - loss: 1.5237 - accuracy: 0.4752\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.5237149000167847, 0.47519999742507935]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.load_model('models/my_cifar10_checkpoint.h5')\n",
    "model.evaluate(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc34cf6e-3789-4cd0-b982-a04e8426cbd1",
   "metadata": {},
   "source": [
    "### c.\n",
    "Now, adding Batch Normalization and compare the learning curve. Is it converging faster than before? Does it produce a better model? How does it affect training speed?\n",
    "\n",
    "The code below is very similar to the code above, with a few changes:\n",
    "\n",
    "- I added a BN layer after every Dense layer (before the activation function), except for the output layer. I also added a BN layer before the first hidden layer.\n",
    "- I changed the learning rate to 5e-4. I experimented with 1e-5, 3e-5, 5e-5, 1e-4, 3e-4, 5e-4, 1e-3 and 3e-3, and I chose the one with the best validation performance after 20 epochs.\n",
    "- I renamed the run directories to runbn* and the model file name to my_cifar10_bn_model.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9fbe32f-3d32-43e9-81e8-06bbefdd9655",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fcba021c0d0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100, kernel_initializer='he_normal'))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Activation('elu'))\n",
    "\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "optimizer = keras.optimizers.Nadam(learning_rate=5e-4)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint('models/my_cifar10_bn_checkpoint.h5', save_best_only=True)\n",
    "run_index = 1\n",
    "run_logdir = os.path.join(os.curdir, \"my_cifar10_logs\", \"run_bn_{:03d}\".format(run_index))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]\n",
    "\n",
    "model.fit(X_train, y_train, epochs=100, validation_data=(X_valid, y_valid), callbacks=callbacks, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d80f4767-5bc1-48e0-be9a-84009c4db0bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 0s 2ms/step - loss: 1.4163 - accuracy: 0.5326\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.4162912368774414, 0.5325999855995178]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3554446d-25d2-4b04-aedc-a0958c835e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 1s 2ms/step - loss: 1.3245 - accuracy: 0.5430\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.3245311975479126, 0.5429999828338623]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.load_model('models/my_cifar10_bn_checkpoint.h5')\n",
    "model.evaluate(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb347e74-b5b3-45b9-9086-a8d08dec235d",
   "metadata": {},
   "source": [
    "### d.\n",
    "Exercise: Try replacing Batch Normalization with SELU, and make the necessary adjustements to ensure the network self-normalizes (i.e., standardize the input features, use LeCun normal initialization, make sure the DNN contains only a sequence of dense layers, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "25e011a5-2f74-4a59-9cf8-171b0acaaa45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 0s 1ms/step - loss: 1.6657 - accuracy: 0.5138\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.6656558513641357, 0.5138000249862671]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100, kernel_initializer=\"lecun_normal\", activation=\"selu\"))\n",
    "    \n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "optimizer = keras.optimizers.Nadam(learning_rate=7e-4)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"models/my_cifar10_selu_model.h5\", save_best_only=True)\n",
    "run_index = 1 # increment every time you train the model\n",
    "run_logdir = os.path.join(os.curdir, \"my_cifar10_logs\", \"run_selu_{:03d}\".format(run_index))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]\n",
    "\n",
    "X_means = X_train.mean(axis=0)\n",
    "X_stds = X_train.std(axis=0)\n",
    "X_train_scaled = (X_train - X_means) / X_stds\n",
    "X_valid_scaled = (X_valid - X_means) / X_stds\n",
    "X_test_scaled = (X_test - X_means) / X_stds\n",
    "\n",
    "model.fit(X_train_scaled, y_train, epochs=100,\n",
    "          validation_data=(X_valid_scaled, y_valid), callbacks=callbacks, verbose=0)\n",
    "\n",
    "#model = keras.models.load_model(\"models/my_cifar10_selu_model.h5\")\n",
    "model.evaluate(X_valid_scaled, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e9cd857-e71e-46e2-9a62-200fb129e735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 0s 1ms/step - loss: 1.4844 - accuracy: 0.4950\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.4844242334365845, 0.4950000047683716]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.load_model(\"models/my_cifar10_selu_model.h5\")\n",
    "model.evaluate(X_valid_scaled, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9977e6-fe65-40c2-9867-0e0fd8137577",
   "metadata": {},
   "source": [
    "### e.\n",
    "Exercise: Try regularizing the model with alpha dropout. Then, without retraining your model, see if you can achieve better accuracy using MC Dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685af252-6066-41ed-97bf-55fc93d4633c",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100,\n",
    "                                 kernel_initializer=\"lecun_normal\",\n",
    "                                 activation=\"selu\"))\n",
    "\n",
    "model.add(keras.layers.AlphaDropout(rate=0.1))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "optimizer = keras.optimizers.Nadam(learning_rate=5e-4)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"models/my_cifar10_alpha_dropout_model.h5\", save_best_only=True)\n",
    "run_index = 1 # increment every time you train the model\n",
    "run_logdir = os.path.join(os.curdir, \"my_cifar10_logs\", \"run_alpha_dropout_{:03d}\".format(run_index))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]\n",
    "\n",
    "X_means = X_train.mean(axis=0)\n",
    "X_stds = X_train.std(axis=0)\n",
    "X_train_scaled = (X_train - X_means) / X_stds\n",
    "X_valid_scaled = (X_valid - X_means) / X_stds\n",
    "X_test_scaled = (X_test - X_means) / X_stds\n",
    "\n",
    "model.fit(X_train_scaled, y_train, epochs=100,\n",
    "          validation_data=(X_valid_scaled, y_valid),\n",
    "          callbacks=callbacks, verbose=0)\n",
    "\n",
    "model = keras.models.load_model(\"models/my_cifar10_alpha_dropout_model.h5\")\n",
    "model.evaluate(X_valid_scaled, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63ba657-0886-461f-bdc6-bd97814b3152",
   "metadata": {},
   "source": [
    "The model reaches 48.9% accuracy on the validation set. That's very slightly better than without dropout (47.6%). With an extensive hyperparameter search, it might be possible to do better (I tried dropout rates of 5%, 10%, 20% and 40%, and learning rates 1e-4, 3e-4, 5e-4, and 1e-3), but probably not much better in this case.\n",
    "\n",
    "Let's use MC Dropout now. We will need the MCAlphaDropout class we used earlier, so let's just copy it here for convenience:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8299914-9a91-40bd-9731-abbec3171c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCAlphaDropout(keras.layers.AlphaDropout):\n",
    "    def call(self, inputs):\n",
    "        return super().call(inputs, training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad48b6e-710b-46dd-b360-f1c81c421ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new model, identical to the one we just trained (with the same weights), but with MCAlphaDropout dropout layers instead of AlphaDropout layers:\n",
    "mc_model = keras.models.Sequential([\n",
    "    MCAlphaDropout(layer.rate) if isinstance(layer, keras.layers.AlphaDropout) else layer\n",
    "    for layer in model.layers\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfccf7a9-3aaa-4a1e-8ac4-8aeb9cf58b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a couple utility functions\n",
    "def mc_dropout_predict_probas(mc_model, X, n_samples=10):\n",
    "    Y_probas = [mc_model.predict(X) for sample in range(n_samples)]\n",
    "    return np.mean(Y_probas, axis=0)\n",
    "\n",
    "def mc_dropout_predict_classes(mc_model, X, n_samples=10):\n",
    "    Y_probas = mc_dropout_predict_probas(mc_model, X, n_samples)\n",
    "    return np.argmax(Y_probas, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96513fde-1e0e-448e-8521-8b3dcbf11d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "y_pred = mc_dropout_predict_classes(mc_model, X_valid_scaled)\n",
    "accuracy = np.mean(y_pred == y_valid[:, 0])\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679d4eb8-599d-4843-b000-83250e14f148",
   "metadata": {},
   "source": [
    "### f.\n",
    "Exercise: Retrain your model using 1cycle scheduling and see if it improves training speed and model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "efe50e1a-2fab-4d38-9eb2-7a25803d3b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100,\n",
    "                                 kernel_initializer=\"lecun_normal\",\n",
    "                                 activation=\"selu\"))\n",
    "\n",
    "model.add(keras.layers.AlphaDropout(rate=0.1))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "optimizer = keras.optimizers.SGD(learning_rate=1e-3)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "880e690a-31cf-40b1-a84a-2386d3f62f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1cycle scheduling\n",
    "import math\n",
    "\n",
    "\n",
    "K = keras.backend\n",
    "\n",
    "class ExponentialLearningRate(keras.callbacks.Callback):\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "        self.rates = []\n",
    "        self.losses = []\n",
    "    def on_batch_end(self, batch, logs):\n",
    "        self.rates.append(K.get_value(self.model.optimizer.learning_rate))\n",
    "        self.losses.append(logs[\"loss\"])\n",
    "        K.set_value(self.model.optimizer.learning_rate, self.model.optimizer.learning_rate * self.factor)\n",
    "\n",
    "def find_learning_rate(model, X, y, epochs=1, batch_size=32, min_rate=10**-5, max_rate=10):\n",
    "    init_weights = model.get_weights()\n",
    "    iterations = math.ceil(len(X) / batch_size) * epochs\n",
    "    factor = np.exp(np.log(max_rate / min_rate) / iterations)\n",
    "    init_lr = K.get_value(model.optimizer.learning_rate)\n",
    "    K.set_value(model.optimizer.learning_rate, min_rate)\n",
    "    exp_lr = ExponentialLearningRate(factor)\n",
    "    history = model.fit(X, y, epochs=epochs, batch_size=batch_size,\n",
    "                        callbacks=[exp_lr])\n",
    "    K.set_value(model.optimizer.learning_rate, init_lr)\n",
    "    model.set_weights(init_weights)\n",
    "    return exp_lr.rates, exp_lr.losses\n",
    "\n",
    "def plot_lr_vs_loss(rates, losses):\n",
    "    plt.plot(rates, losses)\n",
    "    plt.gca().set_xscale('log')\n",
    "    plt.hlines(min(losses), min(rates), max(rates))\n",
    "    plt.axis([min(rates), max(rates), min(losses), (losses[0] + min(losses)) / 2])\n",
    "    plt.xlabel(\"Learning rate\")\n",
    "    plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9925bd53-cbee-482c-bfdf-7a900fbe9084",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "rates, losses = find_learning_rate(model, X_train_scaled, y_train, epochs=1, batch_size=batch_size)\n",
    "plot_lr_vs_loss(rates, losses)\n",
    "plt.axis([min(rates), max(rates), min(losses), (losses[0] + min(losses)) / 1.4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87af477d-7c54-4e97-8f73-108dc59904fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100,\n",
    "                                 kernel_initializer=\"lecun_normal\",\n",
    "                                 activation=\"selu\"))\n",
    "\n",
    "model.add(keras.layers.AlphaDropout(rate=0.1))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "optimizer = keras.optimizers.SGD(learning_rate=1e-2)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f32b01-4368-425d-993a-7584b7962aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 15\n",
    "onecycle = OneCycleScheduler(math.ceil(len(X_train_scaled) / batch_size) * n_epochs, max_rate=0.05)\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs, batch_size=batch_size,\n",
    "                    validation_data=(X_valid_scaled, y_valid),\n",
    "                    callbacks=[onecycle])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b2e9cf-e329-4e95-a631-70f3e1bc90dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
