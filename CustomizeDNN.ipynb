{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a378042-e159-4d57-8308-db88b46726ae",
   "metadata": {},
   "source": [
    "# Customize DNN: Models and Training Algorithms\n",
    "## 1. Custom Loss Function\n",
    "Important note: Save and Load model, saving model is in a normal way, but loading model requires the custom objects that used to map the function name to the actual function.\n",
    "```\n",
    "model = keras.models.load_model(\"my_model_with_a_custom_loss.h5\", custom_objects={\"huber_fn\": huber_fn})\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4475d4e-2b38-4c50-bdd7-b82b29946450",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from scipy import stats\n",
    "import os\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47247178-b7c3-4b6b-b740-e4fdb56807ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load California housing dataset\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target.reshape(-1, 1), test_size=0.1, random_state=42)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, test_size=0.3, random_state=42)\n",
    "\n",
    "# scaling input data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac0ed449-ba28-4f42-9b6c-c495b5d27378",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((13003, 8), (5573, 8), (2064, 8))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled.shape, X_valid_scaled.shape, X_test_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d76c941b-c9a6-446f-ae9e-fbdb6c4b383a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defines Huber Loss function\n",
    "def huber_fn(y_true, y_pred):\n",
    "    error = y_true - y_pred\n",
    "    is_small_error = tf.abs(error) < 1\n",
    "    squared_loss =tf.square(error) / 2\n",
    "    linear_loss = tf.abs(error) - 0.5\n",
    "    return tf.where(is_small_error, squared_loss, linear_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cc15293-e3ec-406c-96e4-78188c8fa7a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-13 11:57:53.410304: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-04-13 11:57:53.701255: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1486 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x252 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8, 3.5))\n",
    "z = np.linspace(-4, 4, 200)\n",
    "plt.plot(z, huber_fn(0, z), \"b-\", linewidth=2, label=\"huber($z$)\")\n",
    "plt.plot(z, z**2 / 2, \"b:\", linewidth=1, label=r\"$\\frac{1}{2}z^2$\")\n",
    "plt.plot([-1, -1], [0, huber_fn(0., -1.)], \"r--\")\n",
    "plt.plot([1, 1], [0, huber_fn(0., 1.)], \"r--\")\n",
    "plt.gca().axhline(y=0, color='k')\n",
    "plt.gca().axvline(x=0, color='k')\n",
    "plt.axis([-4, 4, 0, 4])\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"$z$\")\n",
    "plt.legend(fontsize=14)\n",
    "plt.title(\"Huber loss\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e27a42a-01b7-44d9-a86b-2168e417e9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = X_train.shape[1:]\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n",
    "                       input_shape=input_shape),\n",
    "    keras.layers.Dense(1),\n",
    "])\n",
    "\n",
    "model.compile(loss=huber_fn, optimizer=\"nadam\", metrics=[\"mae\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "feea2b26-c019-4ba4-8ab0-3375ab1a85ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "134/407 [========>.....................] - ETA: 0s - loss: 0.9918 - mae: 1.4262"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-13 11:57:54.911229: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "407/407 [==============================] - 2s 2ms/step - loss: 0.5252 - mae: 0.8756 - val_loss: 0.2335 - val_mae: 0.5252\n",
      "Epoch 2/2\n",
      "407/407 [==============================] - 1s 1ms/step - loss: 0.2187 - mae: 0.5095 - val_loss: 0.2103 - val_mae: 0.4921\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbb680c5280>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3dcde1d-ea85-41ca-ac88-b3a119d1eb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save and load model with custom objects\n",
    "model.save(\"models/my_model_with_a_custom_loss.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e20d387-3d22-4e4d-aa83-47c5e5dfc702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "407/407 [==============================] - 1s 2ms/step - loss: 0.2073 - mae: 0.4940 - val_loss: 0.2033 - val_mae: 0.4848\n",
      "Epoch 2/2\n",
      "407/407 [==============================] - 1s 1ms/step - loss: 0.2020 - mae: 0.4870 - val_loss: 0.2042 - val_mae: 0.4926\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbb6804b9d0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.load_model(\"models/my_model_with_a_custom_loss.h5\",\n",
    "                                custom_objects={\"huber_fn\": huber_fn})\n",
    "\n",
    "# training model again\n",
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "587ad765-16e8-41f3-b357-1bf7a7c0175c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration different threshold for custom loss function\n",
    "def create_huber(threshold=1.0):\n",
    "    def huber_fn(y_true, y_pred):\n",
    "        error = y_true - y_pred\n",
    "        is_small_error = tf.abs(error) < threshold\n",
    "        squared_loss = tf.square(error) / 2\n",
    "        linear_loss = threshold * tf.abs(error) - threshold**2/2\n",
    "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "    return huber_fn\n",
    "\n",
    "model.compile(loss=create_huber(2.0), optimizer='nadam', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f775817d-30e3-460c-bcd2-98ba2ace744d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "407/407 [==============================] - 1s 2ms/step - loss: 0.2311 - mae: 0.4883 - val_loss: 0.2354 - val_mae: 0.4901\n",
      "Epoch 2/2\n",
      "407/407 [==============================] - 1s 1ms/step - loss: 0.2261 - mae: 0.4847 - val_loss: 0.2271 - val_mae: 0.4764\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbb50551fa0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_scaled, y_train, validation_data=(X_valid_scaled, y_valid), epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "146d923d-c6bf-4a9f-aedc-50a930e4b026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/65 [==============================] - 0s 671us/step - loss: 0.2248 - mae: 0.4859\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.22484855353832245, 0.48593869805336]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0be1d0ce-ebca-414d-913e-fb8209beef70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = keras.models.load_model(\"my_model_with_a_custom_loss_threshold_2.h5\", custom_objects={\"huber_fn\": create_huber(2.0)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96d985aa-f7a3-471b-8d32-0fae1b04ce9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can solve this by creating a subclass of the keras.losses.Loss class, and then implementing its get_config() method:\n",
    "class HuberLoss(keras.losses.Loss):\n",
    "    def __init__(self, threshold=1.0, **kwargs):\n",
    "        self.threshold = threshold\n",
    "        super().__init__(**kwargs)\n",
    "    def call(self, y_true, y_pred):\n",
    "        error = y_true - y_pred\n",
    "        is_small_error = tf.abs(error) < self.threshold\n",
    "        squared_loss = tf.square(error) / 2\n",
    "        linear_loss  = self.threshold * tf.abs(error) - self.threshold**2 / 2\n",
    "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"threshold\": self.threshold}\n",
    "\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n",
    "                       input_shape=input_shape),\n",
    "    keras.layers.Dense(1),\n",
    "])\n",
    "\n",
    "model.compile(loss=HuberLoss(2.), optimizer=\"nadam\", metrics=[\"mae\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a04c9992-ff7a-4b59-a0e9-c791ee62aeff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "407/407 [==============================] - 1s 2ms/step - loss: 0.7096 - mae: 0.8807 - val_loss: 0.2766 - val_mae: 0.5331\n",
      "Epoch 2/2\n",
      "407/407 [==============================] - 1s 1ms/step - loss: 0.2567 - mae: 0.5210 - val_loss: 0.2545 - val_mae: 0.5079\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbb68043970>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2420f9f0-cff1-4eee-ad21-99bed0735eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(\"my_model_with_a_custom_loss_class.h5\")\n",
    "# model = keras.models.load_model(\"my_model_with_a_custom_loss_class.h5\", custom_objects={\"HuberLoss\": HuberLoss})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6192715a-d309-48a6-9f7a-74f9fc92a195",
   "metadata": {},
   "source": [
    "The constructor passes \\**kwargs to the parent constructor by super().\\__init__(\\**kwargs). \n",
    "\n",
    "The call() method takes the labels and predictions, computes all the instance losses, and return them\n",
    "\n",
    "The get_config() method return a dictionary mapping each hyperparameter name to its values.\n",
    "\n",
    "# Other custom Functions: custom Activation functions, Initializers, Regularizers, and Constraints\n",
    "Most Keras functionalities, such as losses, regularizers, constraints, initializers, metrics, activation functions, layers, and even full models, can be customized in very much the same way. Most of the time, you will just need to write a simple function with the appropriate inputs and outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "345dafcc-ac5f-4f68-999a-3d9d5006919c",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# customize \n",
    "def my_softplus(z): # return value is just tf.nn.softplus(z)\n",
    "    return tf.math.log(tf.exp(z) + 1.0)\n",
    "\n",
    "def my_glorot_initializer(shape, dtype=tf.float32):\n",
    "    stddev = tf.sqrt(2. / (shape[0] + shape[1]))\n",
    "    return tf.random.normal(shape, stddev=stddev, dtype=dtype)\n",
    "\n",
    "def my_l1_regularizer(weights):\n",
    "    return tf.reduce_sum(tf.abs(0.01 * weights))\n",
    "\n",
    "def my_positive_weights(weights): # return value is just tf.nn.relu(weights)\n",
    "    return tf.where(weights < 0., tf.zeros_like(weights), weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e2f5212-8ca2-4de9-aaa2-d7573a4ce2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "## using customized functions\n",
    "# layer = keras.layers.Dense(1, activation=my_softplus,\n",
    "#                           kernel_initializer=my_glorot_initializer,\n",
    "#                           kernel_regularizer=my_l1_regularizer,\n",
    "#                           kernel_constraint=my_positive_weights)              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "802b7c6a-6ff3-4f68-b94e-f4ecfed8c43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n",
    "                       input_shape=input_shape),\n",
    "    keras.layers.Dense(1, activation=my_softplus,\n",
    "                       kernel_regularizer=my_l1_regularizer,\n",
    "                       kernel_constraint=my_positive_weights,\n",
    "                       kernel_initializer=my_glorot_initializer),\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=\"nadam\", metrics=[\"mae\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f2f6a058-6d3b-4b20-919f-daca2b912430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "407/407 [==============================] - 1s 2ms/step - loss: 1.5735 - mae: 0.8734 - val_loss: 0.6552 - val_mae: 0.5444\n",
      "Epoch 2/2\n",
      "407/407 [==============================] - 1s 1ms/step - loss: 0.6401 - mae: 0.5259 - val_loss: 0.5321 - val_mae: 0.4991\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbb502016a0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f82762f-d14f-43bd-8430-135184435f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## save and load model with all customized functions\n",
    "\n",
    "#model.save(\"my_model_with_many_custom_parts.h5\")\n",
    "\n",
    "# model = keras.models.load_model(\"my_model_with_many_custom_parts.h5\",\n",
    "#    custom_objects={\n",
    "#       \"my_l1_regularizer\": my_l1_regularizer,\n",
    "#       \"my_positive_weights\": my_positive_weights,\n",
    "#       \"my_glorot_initializer\": my_glorot_initializer,\n",
    "#       \"my_softplus\": my_softplus,\n",
    "#    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f19fbab8-2094-4732-8d9e-b564f1388ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## given hyperparameter to function and saving it with model by defining a subclass the approriate class, such as keras.regularizers.Regularizer, \n",
    "## keras.constraints.Constraint, keras.initializers.Initializer\n",
    "\n",
    "class MyL1Regularizer(keras.regularizers.Regularizer):\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "    def __call__(self, weights):\n",
    "        return tf.reduce_sum(tf.abs(self.factor * weights))\n",
    "    def get_config(self):\n",
    "        return {\"factor\": self.factor}\n",
    "\n",
    "## implement the call() method for losses, layers (including activation functions), and models\n",
    "## the __call__() method for regularizers, initializers, and constraints. For metrics, things are a bit different,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b9ec37a0-1c74-4b27-80f3-67ede01e0cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n",
    "                       input_shape=input_shape),\n",
    "    keras.layers.Dense(1, activation=my_softplus,\n",
    "                       kernel_regularizer=MyL1Regularizer(0.01),\n",
    "                       kernel_constraint=my_positive_weights,\n",
    "                       kernel_initializer=my_glorot_initializer),\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=\"nadam\", metrics=[\"mae\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4e9dc1a5-b45b-4424-be0f-bbf6e7d33472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "407/407 [==============================] - 1s 2ms/step - loss: 1.5735 - mae: 0.8734 - val_loss: 0.6552 - val_mae: 0.5444\n",
      "Epoch 2/2\n",
      "407/407 [==============================] - 1s 1ms/step - loss: 0.6401 - mae: 0.5259 - val_loss: 0.5321 - val_mae: 0.4991\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbb5004bd60>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7a11f22b-9295-4465-808e-400fff9d336b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load model\n",
    "#model = keras.models.load_model(\"my_model_with_many_custom_parts.h5\",\n",
    "#    custom_objects={\n",
    "#       \"MyL1Regularizer\": MyL1Regularizer,\n",
    "#       \"my_positive_weights\": my_positive_weights,\n",
    "#       \"my_glorot_initializer\": my_glorot_initializer,\n",
    "#       \"my_softplus\": my_softplus,\n",
    "#    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc23244a-3e9a-4998-a435-98a2cfb6d230",
   "metadata": {},
   "source": [
    "# Custom Metrics\n",
    "*Losses and Metrics* are conceptually not the same thing:\n",
    "- Losses (e.g., cross entropy) are used by Gradient Descent to train model, so they must differentiable (at least where they arre evaluated), and their gradient should not be 0 anywhere. It is not easy interpretable by humans.\n",
    "- Metrics (e.g., accuracy) are used to evaluate a model: they must be more easily interpretable, and they can be non-differentiable or have 0 gradient everywhere.\n",
    "\n",
    "In some cases, defining custom metrics is exactly the same as defining a custom loss function. ```model.compile(loss=\"mse\", optimizer=\"nadam\", metrics=[create_huber(2.0)])``` (using the create_huber class of loss function for custom metrics is fine)\n",
    "\n",
    "**Note:** if you use the same function as the loss and a metric, you may be surprised to see different results. This is generally just due to floating point precision errors: even though the mathematical equations are equivalent, the operations are not run in the same order, which can lead to small differences. Moreover, when using sample weights, there's more than just precision errors:\n",
    "\n",
    "- the loss since the start of the epoch is the mean of all batch losses seen so far. Each batch loss is the sum of the weighted instance losses divided by the batch size (not the sum of weights, so the batch loss is not the weighted mean of the losses).\n",
    "- the metric since the start of the epoch is equal to the sum of weighted instance losses divided by sum of all weights seen so far. In other words, it is the weighted mean of all the instance losses. Not the same thing.\n",
    "\n",
    "If you do the math, you will find that loss = metric * mean of sample weights (plus some floating point precision error).\n",
    "\n",
    "### Streaming metric: Precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bc72a18b-ed73-4b33-9eb7-809b291633fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.8>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision = keras.metrics.Precision()\n",
    "# first batch\n",
    "precision([0, 1, 1, 1, 0, 1, 0, 1], [1, 1, 0, 1, 0, 1, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d8ad7a42-1751-4bc5-9ded-dd4333457382",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.5>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# second batch\n",
    "precision([0, 1, 0, 0, 1, 0, 1, 1], [1, 0, 1, 1, 0, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9d8d8e3d-3b34-4275-a38e-fba4ad4dbb34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.5>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get current value of the metric\n",
    "precision.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "55dde76a-c3db-4993-b5e6-2f7308f93b83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'true_positives:0' shape=(1,) dtype=float32, numpy=array([4.], dtype=float32)>,\n",
       " <tf.Variable 'false_positives:0' shape=(1,) dtype=float32, numpy=array([4.], dtype=float32)>]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# variables of precision\n",
    "precision.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9078a009-d0bc-4a41-8054-dcb41af03f09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'true_positives:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n",
       " <tf.Variable 'false_positives:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reset all variables \n",
    "precision.reset_states()\n",
    "precision.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2764474a-97d1-4697-a5f0-b6fc1c4598b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# customize subclass metrics from keras.metrics.Metric\n",
    "class HuberMetric(keras.metrics.Metric):\n",
    "    \n",
    "    def __init__(self, threshold=1.0, **kwargs):\n",
    "        super().__init__(**kwargs) # handles base args (e.g., dtype)\n",
    "        self.threshold = threshold\n",
    "        self.huber_fn = create_huber(threshold)\n",
    "        self.total = self.add_weight(\"total\", initializer=\"zeros\")\n",
    "        self.count = self.add_weight(\"count\", initializer=\"zeros\")\n",
    "    \n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        metric = self.huber_fn(y_true, y_pred)\n",
    "        self.total.assign_add(tf.reduce_sum(metric))\n",
    "        self.count.assign_add(tf.cast(tf.size(y_true), tf.float32))\n",
    "    \n",
    "    def result(self):\n",
    "        return self.total / self.count\n",
    "    \n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"threshold\": self.threshold}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d941f5-19b0-432c-8a49-e409ca6ed44d",
   "metadata": {},
   "source": [
    "- The constructor use the add_weight() to create the variables needed to keep track of the metric's state over multiple batches. \"total\" is the sum of all Huber losses, and \"count\" is the number of instances seen so far.\n",
    "- The update_state() to update all variables, gives the labels, and predictions for one batch.\n",
    "- The result() computes and returns the final result, in that case, the Huber metric over all instances\n",
    "- The get_config() save the threshold along with the model. If we use this class as function, the update_state() is called first, and then the result() to return a result.\n",
    "- The default implementation of the reset_states() method resets all variables to 0.0, but we can override it if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f99d4971-6f1d-48d7-ac3e-b488bb717e6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=14.0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use as function\n",
    "m = HuberMetric(2.)\n",
    "m(tf.constant([[2.]]), tf.constant([[10.]]))\n",
    "\n",
    "## it computes:\n",
    "# total = 2 * |10 - 2| - 2/2 = 14\n",
    "# count = 1\n",
    "# result = 14 / 1 = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3c6e1b45-f453-4a35-abc5-c80fbbba0821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=7.0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total = total + (|1 - 0| / 2) + (2 * |9.25 - 5| - 2 / 2) = 14 + 7 = 21\n",
    "# count = count + 2 = 3\n",
    "# result = total / count = 21 / 3 = 7\n",
    "m(tf.constant([[0.], [5.]]), tf.constant([[1.], [9.25]]))\n",
    "\n",
    "m.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b81b45b0-24f5-4b38-b37b-ec99b61df304",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'total:0' shape=() dtype=float32, numpy=21.0>,\n",
       " <tf.Variable 'count:0' shape=() dtype=float32, numpy=3.0>]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "84e147c6-8e0d-484e-9a20-a36e9bd07bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the HuberMetric work\n",
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n",
    "                       input_shape=input_shape),\n",
    "    keras.layers.Dense(1),\n",
    "])\n",
    "\n",
    "model.compile(loss=create_huber(2.0), optimizer=\"nadam\", metrics=[HuberMetric(2.0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1ffc07ce-ba14-4ec9-a939-0136996faf8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "407/407 [==============================] - 1s 1ms/step - loss: 0.8106 - huber_metric: 0.8106\n",
      "Epoch 2/2\n",
      "407/407 [==============================] - 0s 1ms/step - loss: 0.2524 - huber_metric: 0.2524\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbb486afe80>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_scaled.astype(np.float32), y_train.astype(np.float32), epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e5032d00-9c0c-47ef-a733-a701947ea751",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"models/my_model_with_huber_metric.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "13f05c2e-770d-4fb8-b922-679365014cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"models/my_model_with_huber_metric.h5\", custom_objects={\"huber_fn\":create_huber(2.0), \"HuberMetric\":HuberMetric()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "efe5869c-bda9-43a8-b8e5-0f7c1af8b6d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "407/407 [==============================] - 1s 1ms/step - loss: 0.2373 - huber_metric: 0.2373\n",
      "Epoch 2/2\n",
      "407/407 [==============================] - 0s 1ms/step - loss: 0.2307 - huber_metric: 0.2307\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbb486455b0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_scaled.astype(np.float32), y_train.astype(np.float32), epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3f60eb6b-7852-42e9-8db9-b9144a792b27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.metrics[-1].threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a3be49a5-96a2-4ac7-8bf6-4ecb8dcfb158",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.metrics.Mean at 0x7fbb4852d190>,\n",
       " <__main__.HuberMetric at 0x7fbb48527880>]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6eb116-1c96-412e-b762-455420793d27",
   "metadata": {},
   "source": [
    "# Custom Layers\n",
    "the simplest option is to write a function and wrap it in a ```keras.layers.Lambda layer```. For example, the following layer will apply the exponential function to its inputs:\n",
    "\n",
    "The custom layer can be used like any other layer, using Sequential API, Functional API, or the Subclassing API, and also use it as an activation function (for Exponential layer).\n",
    "\n",
    "Custom a subclass of ```keras.layers.Layer``` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "589d8819-a6a9-40ad-a77a-4725a75fb92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDense(keras.layers.Layer):\n",
    "    def __init__(self, units, activation=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = keras.activations.get(activation)\n",
    "\n",
    "    def build(self, batch_input_shape):\n",
    "        self.kernel = self.add_weight(\n",
    "            name=\"kernel\", shape=[batch_input_shape[-1], self.units],\n",
    "            initializer=\"glorot_normal\")\n",
    "        self.bias = self.add_weight(\n",
    "            name=\"bias\", shape=[self.units], initializer=\"zeros\")\n",
    "        super().build(batch_input_shape) # must be at the end\n",
    "\n",
    "    def call(self, X):\n",
    "        return self.activation(X @ self.kernel + self.bias)\n",
    "\n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        return tf.TensorShape(batch_input_shape.as_list()[:-1] + [self.units])\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"units\": self.units,\n",
    "                \"activation\": keras.activations.serialize(self.activation)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc859c32-9324-483d-a34f-939d5e2d0c69",
   "metadata": {
    "tags": []
   },
   "source": [
    "- The constructor takes all the hyperparameters as arguments (e.g., input units, activation), take care of standard arguments such as **input_shape, trainable, and name**. Save all hyperparameters as attributes, converting the activation argument to the approriate activation function by using the ```keras.activations.get()``` method.\n",
    "- The build() method's role is to create the layer's variables by calling the add_weight() method for each weight. This method is called first when the layer is used. At this point, Keras knew the shape of previous neurons by the ```batch_input_shape``` argument to establish connections of weights metrix (i.e., kernel). At the end of this method, must call the parent's build() method to tell Keras builds this layer by setting ```self.build=True```.\n",
    "- The call() method performs the desired operations. We just compute the input X and the layer's kernel, adding the bias vector, and then apply the activation function to the result, and this give us the output of the layer.\n",
    "- The compute_output_shape() method simply returns the shape of this layer's outputs. The shape is the same as input's shape (by batch_input_shape argument) and can convert to Python lists by using as_list(). \n",
    "- The get_config() method to comfirm everything is done for this layer, and calling the ```keras.activations.serialize()``` to save the units, and activation arguments\n",
    "\n",
    "You can generally omit the compute_output_shape() method, as tf.keras automatically infers the output shape, except when the layer is dynamic (as we will see shortly). In other Keras implementations, this method is either required or its default implementation assumes the output shape is the same as the input shape.\n",
    "\n",
    "#### Multiple inputs (eg., Concatenate)\n",
    "The argument the the call() method should be a tuple containing all the inputs, and similarly the argument to the compute_output_shape() method should be a tuple containing each input's batch shape.\n",
    "```\n",
    "class MyMultiLayer(keras.layers.Layer):\n",
    "    def call(self, X):\n",
    "        X1, X2 = X\n",
    "        return [X1 + X2, X1 * X2, X1 / X2]\n",
    "\n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        b1, b2 = batch_input_shape\n",
    "        return [b1, b1, b1] # should probably handle broadcasting rules\n",
    "```\n",
    "This layer may now be used like any other layer, but only using the Functional and Subclassing APIs, not the Sequential API (which only accepts layers with one input and one output).\n",
    "\n",
    "If your layer needs to have a different behavior during training and during testing (e.g., if it uses Dropout or BatchNormalization layers), then you must add a train ing argument to the call() method and use this argument to decide what to do. For example, lets create a layer that adds Gaussian noise during training (for regularization) but does nothing during testing (Keras has a layer that does the same thing, keras.layers.GaussianNoise ):\n",
    "```\n",
    "class MyGaussianNoise(keras.layers.Layer):\n",
    "    def __init__(self, stddev, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.stddev = stddev\n",
    "\n",
    "    def call(self, X, training=None):\n",
    "        if training:\n",
    "            noise = tf.random.normal(tf.shape(X), stddev=self.stddev)\n",
    "            return X + noise\n",
    "        else:\n",
    "            return X\n",
    "            \n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        return batch_input_shape\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "00f92492-35dd-4da0-a25c-460fe9aa1000",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    MyDense(30, activation=\"relu\", input_shape=input_shape),\n",
    "    MyDense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9e50f2b4-bd35-43f3-b982-075182830736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "407/407 [==============================] - 1s 2ms/step - loss: 0.5208 - accuracy: 0.0028 - val_loss: 0.4563 - val_accuracy: 0.0038\n",
      "Epoch 2/2\n",
      "407/407 [==============================] - 1s 1ms/step - loss: 0.4433 - accuracy: 0.0029 - val_loss: 0.4158 - val_accuracy: 0.0034\n",
      "65/65 [==============================] - 0s 637us/step - loss: 0.4535 - accuracy: 9.6899e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4535139501094818, 0.0009689922444522381]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=\"mse\", optimizer=\"nadam\", metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))\n",
    "\n",
    "model.evaluate(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4dc6162d-11d1-42aa-8805-116d976e1199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiple layer\n",
    "class MyMultiLayer(keras.layers.Layer):\n",
    "    def call(self, X):\n",
    "        X1, X2 = X\n",
    "        print(\"X1.shape: \", X1.shape ,\" X2.shape: \", X2.shape) # Debugging of custom layer\n",
    "        return X1 + X2, X1 * X2\n",
    "\n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        batch_input_shape1, batch_input_shape2 = batch_input_shape\n",
    "        return [batch_input_shape1, batch_input_shape2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "850a0bea-5eb8-4841-a355-61d719122db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X1.shape:  (None, 2)  X2.shape:  (None, 2)\n"
     ]
    }
   ],
   "source": [
    "# our custom layer can be called using the Functional API like this:\n",
    "\n",
    "inputs1 = keras.layers.Input(shape=[2])\n",
    "inputs2 = keras.layers.Input(shape=[2])\n",
    "outputs1, outputs2 = MyMultiLayer()((inputs1, inputs2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e65891f-de1f-4973-9d28-73d1aa1b8a59",
   "metadata": {},
   "source": [
    "Define a Functional model using the multiple layer\n",
    "```\n",
    "input_A = keras.layers.Input(shape=X_train_scaled_A.shape[-1])\n",
    "input_B = keras.layers.Input(shape=X_train_scaled_B.shape[-1])\n",
    "hidden_A, hidden_B = MyMultiLayer()((input_A, input_B))\n",
    "hidden_A = keras.layers.Dense(30, activation='selu')(hidden_A)\n",
    "hidden_B = keras.layers.Dense(30, activation='selu')(hidden_B)\n",
    "concat = keras.layers.Concatenate()((hidden_A, hidden_B))\n",
    "output = keras.layers.Dense(1)(concat)\n",
    "model = keras.models.Model(inputs=[input_A, input_B], outputs=[output])\n",
    "```\n",
    "\n",
    "# Custom Models\n",
    "subclass the keras.Model class, create layers and variables in the constructor, and implement the call() method to do whatever you want the model to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2611748b-2b88-4d26-924a-1de0ba80ae7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a ResidualBlock class (a layer), that composes of two dense layers, and an addition operation (add its inputs to its outputs), then do 3 loops of the ResidualBlock, \n",
    "## and goes through a second ResidualBlock. The final result goes through the dense output layer.\n",
    "\n",
    "class ResidualBlock(keras.layers.Layer):\n",
    "    def __init__(self, n_layers, n_neurons, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden = [keras.layers.Dense(n_neurons, activation=\"elu\", kernel_initializer=\"he_normal\") for _ in range(n_layers)]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        Z = inputs\n",
    "        for layer in self.hidden:\n",
    "            Z = layer(Z)\n",
    "        return inputs + Z\n",
    "    \n",
    "## This layer is a bit special since it contains other layers. This is automatically handled Transparently by Keras. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "81793573-13c8-48ab-8252-40aa5f957d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## next, use the Subclassing API to define the model itself.\n",
    "\n",
    "class ResidualRegressor(keras.models.Model):\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden1 = keras.layers.Dense(30, activation=\"elu\",\n",
    "                                          kernel_initializer=\"he_normal\")\n",
    "        self.block1 = ResidualBlock(2, 30)\n",
    "        self.block2 = ResidualBlock(2, 30)\n",
    "        self.out = keras.layers.Dense(output_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        Z = self.hidden1(inputs)\n",
    "        for _ in range(1 + 3):\n",
    "            Z = self.block1(Z)\n",
    "        Z = self.block2(Z)\n",
    "        return self.out(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f6f564b4-a894-4bef-9a66-63e28d0abbe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "407/407 [==============================] - 1s 2ms/step - loss: 12.5406\n",
      "Epoch 2/5\n",
      "407/407 [==============================] - 1s 2ms/step - loss: 3.0691\n",
      "Epoch 3/5\n",
      "407/407 [==============================] - 1s 2ms/step - loss: 1.0037\n",
      "Epoch 4/5\n",
      "407/407 [==============================] - 1s 2ms/step - loss: 0.8037\n",
      "Epoch 5/5\n",
      "407/407 [==============================] - 1s 2ms/step - loss: 0.7256\n",
      "65/65 [==============================] - 0s 800us/step - loss: 0.5307\n"
     ]
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "X_new_scaled = X_test_scaled\n",
    "\n",
    "# simple using, just create and call\n",
    "model = ResidualRegressor(1)\n",
    "model.compile(loss=\"mse\", optimizer=\"nadam\")\n",
    "history = model.fit(X_train_scaled, y_train, epochs=5)\n",
    "score = model.evaluate(X_test_scaled, y_test)\n",
    "y_pred = model.predict(X_new_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fcbbe990-ab5d-4e8c-b6a7-dc38ad248c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-13 16:05:32.550557: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:absl:Found untraced functions such as dense_1_layer_call_fn, dense_1_layer_call_and_return_conditional_losses, dense_2_layer_call_fn, dense_2_layer_call_and_return_conditional_losses, dense_3_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/my_custom_model.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/my_custom_model.ckpt/assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"models/my_custom_model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d71846bd-7b50-4e90-b15c-9087fddba470",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"models/my_custom_model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f3412110-f26d-4327-bdc3-4f9775c6db3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "407/407 [==============================] - 1s 2ms/step - loss: 0.6891\n",
      "Epoch 2/2\n",
      "407/407 [==============================] - 1s 2ms/step - loss: 0.6294\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbb1820ee50>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_scaled, y_train, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "aef64b54-ce97-4a84-b9e8-1a280992ae90",
   "metadata": {},
   "outputs": [],
   "source": [
    "## define the model using the Sequential API instead.\n",
    "\n",
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "block1 = ResidualBlock(2, 30)\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    block1, block1, block1, block1, # we can add many hidden layers\n",
    "    ResidualBlock(2, 30),\n",
    "    keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8885ff55-cfc8-4d37-ab5a-30dfc7def87a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "407/407 [==============================] - 1s 2ms/step - loss: 0.8066\n",
      "Epoch 2/5\n",
      "407/407 [==============================] - 1s 2ms/step - loss: 0.5595\n",
      "Epoch 3/5\n",
      "407/407 [==============================] - 1s 2ms/step - loss: 0.4903\n",
      "Epoch 4/5\n",
      "407/407 [==============================] - 1s 2ms/step - loss: 1.7035\n",
      "Epoch 5/5\n",
      "407/407 [==============================] - 1s 2ms/step - loss: 0.5178\n",
      "65/65 [==============================] - 0s 700us/step - loss: 0.3763\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_2 (Dense)             (None, 30)                270       \n",
      "                                                                 \n",
      " residual_block (ResidualBlo  (None, 30)               1860      \n",
      " ck)                                                             \n",
      "                                                                 \n",
      " residual_block_1 (ResidualB  (None, 30)               1860      \n",
      " lock)                                                           \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 31        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,021\n",
      "Trainable params: 4,021\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"mse\", optimizer=\"nadam\")\n",
    "history = model.fit(X_train_scaled, y_train, epochs=5)\n",
    "score = model.evaluate(X_test_scaled, y_test)\n",
    "y_pred = model.predict(X_new_scaled)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ed50f1-6e6d-4c22-af57-a1a6ca1b72bc",
   "metadata": {},
   "source": [
    "## Losses and Metrics Based on model internals\n",
    "To define a custom loss based on model internals, compute it based on any part of the model you want, then pass the result to the add_loss() method.\n",
    "\n",
    "Similarly, you can add a custom metric based on model internals by computing it in any way you want, as long as the result is the output of a metric object. For example, you can create a keras.metrics.Mean object in the constructor, then call it in the call() method, passing it the recon_loss , and finally add it to the model by calling the models add_metric() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "39262552-98cd-40cf-81f3-b7fabe0138da",
   "metadata": {},
   "outputs": [],
   "source": [
    "## define the model, add custom loss and metric\n",
    "class ReconstructingRegressor(keras.Model):\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden = [keras.layers.Dense(30, activation=\"selu\",\n",
    "                                          kernel_initializer=\"lecun_normal\")\n",
    "                       for _ in range(5)]\n",
    "        self.out = keras.layers.Dense(output_dim)\n",
    "        self.reconstruction_mean = keras.metrics.Mean(name=\"reconstruction_error\")\n",
    "\n",
    "    def build(self, batch_input_shape):\n",
    "        n_inputs = batch_input_shape[-1]\n",
    "        self.reconstruct = keras.layers.Dense(n_inputs)\n",
    "        #super().build(batch_input_shape)\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        Z = inputs\n",
    "        for layer in self.hidden:\n",
    "            Z = layer(Z)\n",
    "        reconstruction = self.reconstruct(Z)  # pass layers to reconstruct\n",
    "        # customize loss result\n",
    "        recon_loss = tf.reduce_mean(tf.square(reconstruction - inputs))\n",
    "        # store result into model's list of losses\n",
    "        self.add_loss(0.05 * recon_loss)\n",
    "        if training:\n",
    "            # customize the metric and store its result into the model\n",
    "            result = self.reconstruction_mean(recon_loss)\n",
    "            self.add_metric(result)\n",
    "        return self.out(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168c8b68-dfb7-4f67-9396-c81ce3d8e9b6",
   "metadata": {},
   "source": [
    "- The Model constructor create the DNN with 5 dense hidden layers and one dense output player\n",
    "- build() method creates an extra dense layer which will be used to reconstruct the input of the model. We create it here because its number of units must be equal to the number of inputs, and this number is unknown before the build() method is called.\n",
    "- The call() method process inputs through all 5 layers, then pass the result through the reconstruction layer, which produces the reconstruction.\n",
    "- Then, it computes the reconstruct loss by mean squared difference between reconstruct and the inputs. then, add the result to the model's list of losses by add_loss() method.\n",
    "- Then, we track the Mean in the reconstruction loss and adding it to model's list metric by add_metric() method.\n",
    "- Finally, the call() passes the output of the hidden layers to the output layer and returns its output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5d070e23-4583-4cb7-8a38-3115fdd430ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "407/407 [==============================] - 1s 2ms/step - loss: 0.7774 - reconstruction_error: 0.9640\n",
      "Epoch 2/2\n",
      "407/407 [==============================] - 1s 2ms/step - loss: 0.4352 - reconstruction_error: 0.4065\n"
     ]
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model = ReconstructingRegressor(1)\n",
    "model.compile(loss=\"mse\", optimizer=\"nadam\")\n",
    "history = model.fit(X_train_scaled, y_train, epochs=2)\n",
    "y_pred = model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6196d88b-114d-46cb-b37a-e7b06cc0f02e",
   "metadata": {},
   "source": [
    "## Computing Gradients using Autodiff\n",
    "To compute gradients automatically (find the difference of partial derivative of the function). In NN, finding corresponding the partial devirative of each neuron is impossible task. One solution could be to compute an approximation of each partial derivative by measuring how much the function's output changes when you tweak the corresponding parameter:\n",
    "\n",
    "In TF so easy, using ```tf.GradientTape``` context\n",
    "```\n",
    "w1, w2 = tf.Variable(5.), tf.Variable(3.)\n",
    "with tf.GradientTape() as tape:\n",
    "    z = f(w1, w2)\n",
    "\n",
    "gradients = tape.gradient(z, [w1, w2])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bd53435-d4ba-47f7-a9dc-6321f2090996",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(w1, w2):\n",
    "    return 3 * w1**2 + 2 * w1 * w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f659d606-0660-45d3-b2a1-c106578f09f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-14 11:50:49.792089: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-04-14 11:50:50.083414: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1745 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "w1, w2 = tf.Variable(5.), tf.Variable(3.)\n",
    "with tf.GradientTape() as tape:\n",
    "    z = f(w1, w2)\n",
    "\n",
    "gradients = tape.gradient(z, [w1, w2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f610f665-71d1-454b-bbef-ac3e2af5429c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=36.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=10.0>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25428138-b3d1-4624-9a20-2abc92c6a5b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A non-persistent GradientTape can only be used to compute one set of gradients (or jacobians)\n"
     ]
    }
   ],
   "source": [
    "## default, the Tape only run once. if we call gradients() twice, runtime error\n",
    "with tf.GradientTape() as tape:\n",
    "    z = f(w1, w2)\n",
    "\n",
    "dz_dw1 = tape.gradient(z, w1)\n",
    "try:\n",
    "    dz_dw2 = tape.gradient(z, w2)\n",
    "except RuntimeError as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48c3cce2-c831-48a5-9db6-f9c811dd1a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to run it more than one time, setting persistent=True, and then delete tape\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    z = f(w1, w2)\n",
    "\n",
    "dz_dw1 = tape.gradient(z, w1)\n",
    "dz_dw2 = tape.gradient(z, w2) # works now!\n",
    "del tape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02a1149a-28e2-438c-866a-7234bb15f373",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=float32, numpy=36.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=10.0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dz_dw1, dz_dw2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e63846c-ca2d-4e79-8a6a-6ae83fda7042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the Tap only track operation involving variables\n",
    "c1, c2 = tf.constant(5.), tf.constant(3.)\n",
    "with tf.GradientTape() as tape:\n",
    "    z = f(c1, c2)\n",
    "\n",
    "gradients = tape.gradient(z, [c1, c2])\n",
    "gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5935213-d30c-4a83-9531-2f3017c391c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=36.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=10.0>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Solution,\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(c1)\n",
    "    tape.watch(c2)\n",
    "    z = f(c1, c2)\n",
    "\n",
    "gradients = tape.gradient(z, [c1, c2])\n",
    "gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88216d8b-0a8f-4272-b848-7d8a3630109b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=136.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=30.0>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Multiple tape functions\n",
    "with tf.GradientTape() as tape:\n",
    "    z1 = f(w1, w2 + 2.)\n",
    "    z2 = f(w1, w2 + 5.)\n",
    "    z3 = f(w1, w2 + 7.)\n",
    "\n",
    "tape.gradient([z1, z2, z3], [w1, w2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12b511d3-2c12-4a90-8b73-5fdeef2d5f39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([136.,  30.], dtype=float32)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## another sum\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    z1 = f(w1, w2 + 2.)\n",
    "    z2 = f(w1, w2 + 5.)\n",
    "    z3 = f(w1, w2 + 7.)\n",
    "    \n",
    "gradients = tf.reduce_sum(tf.stack([tape.gradient(z, [w1, w2]) for z in (z1, z2, z3)]), axis=0)\n",
    "del tape\n",
    "\n",
    "gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "001e2adb-dd4f-42f7-a90f-32cf622800ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "## compute gradients for a vector, ex. vector contains multiple losses\n",
    "\n",
    "with tf.GradientTape(persistent=True) as hessian_tape:\n",
    "    with tf.GradientTape() as jacobian_tape:\n",
    "        z = f(w1, w2)\n",
    "    jacobians = jacobian_tape.gradient(z, [w1, w2])\n",
    "# uses the vector jacobians\n",
    "hessians = [hessian_tape.gradient(jacobian, [w1, w2])\n",
    "            for jacobian in jacobians]\n",
    "del hessian_tape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39492714-243c-482b-bdf1-61a3e20b2587",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=36.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=10.0>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jacobians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a471507b-946f-4b30-9203-eb4de5e9ebe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[<tf.Tensor: shape=(), dtype=float32, numpy=6.0>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=2.0>],\n",
       " [<tf.Tensor: shape=(), dtype=float32, numpy=2.0>, None]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hessians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2e7f5fa9-2550-46b8-9934-564730f13f9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=30.0>, None]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## using stop_gradient() to cancel computing of gradient\n",
    "def f(w1, w2):\n",
    "    return 3 * w1 ** 2 + tf.stop_gradient(2 * w1 * w2)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    z = f(w1, w2)\n",
    "\n",
    "tape.gradient(z, [w1, w2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9e043ee-c390-4e8b-ad36-274fc062a048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1,), dtype=float32, numpy=array([inf], dtype=float32)>,\n",
       " [<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## using annotation telling to TF uses the stable function when computing the gradients of the my_softplus() and return both the normal output and deviratives\n",
    "@tf.custom_gradient\n",
    "def my_better_softplus(z):\n",
    "    exp = tf.exp(z)\n",
    "    def my_softplus_gradients(grad):\n",
    "        return grad / (1 + 1 / exp)\n",
    "    return tf.math.log(exp + 1), my_softplus_gradients\n",
    "\n",
    "x = tf.Variable([1000.])\n",
    "with tf.GradientTape() as tape:\n",
    "    z = my_better_softplus(x)\n",
    "\n",
    "z, tape.gradient(z, [x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "271f4ffb-e99d-409a-a122-5cfb5c159728",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1000.], dtype=float32)>,\n",
       " [<tf.Tensor: shape=(1,), dtype=float32, numpy=array([nan], dtype=float32)>])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_better_softplus(z):\n",
    "    return tf.where(z > 30., z, tf.math.log(tf.exp(z) + 1.))\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    z = my_better_softplus(x)\n",
    "\n",
    "z, tape.gradient(z, [x])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c99b74-56ea-4262-a56e-f9f831893f50",
   "metadata": {},
   "source": [
    "## Custom Training Loops\n",
    "fit() function is not flexible, it only use one Optimizer. but in case, if we want to use two optimizers for two diffrent paths.\n",
    "\n",
    "Unless you really need the extra flexibility, you should prefer using the fit() method rather than implementing your own training loop, especially if you work in a team."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "53ee2297-8278-445c-b13f-135edee9bdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "## First, we build the model.\n",
    "l2_reg = keras.regularizers.l2(0.05)\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"elu\", kernel_initializer=\"he_normal\",\n",
    "                       kernel_regularizer=l2_reg),\n",
    "    keras.layers.Dense(1, kernel_regularizer=l2_reg)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e28b1f32-5b93-4570-971d-bcaa546a59f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Next, create a tiny function that will randomly sample a batch of instances from the training set.\n",
    "def random_batch(X, y, batch_size=32):\n",
    "    idx = np.random.randint(len(X), size=batch_size)\n",
    "    return X[idx], y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "27f81e16-fc55-43bc-bb27-22cc830bb926",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define a function to display the training status, including the number of steps, and so on.\n",
    "def print_status_bar(iteration, total, loss, metrics=None):\n",
    "    metrics = \" - \".join([\"{}: {:.4f}\".format(m.name, m.result())\n",
    "                         for m in [loss] + (metrics or [])])\n",
    "    end = \"\" if iteration < total else \"\\n\"\n",
    "    print(\"\\r{}/{} - \".format(iteration, total) + metrics,\n",
    "          end=end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b99df6df-c69a-40b5-be59-aeb6788518b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define some hyperparameters, choose the optimizer, loss function, the metrics\n",
    "n_epochs = 5\n",
    "batch_size = 32\n",
    "n_steps = len(X_train) // batch_size\n",
    "optimizer = keras.optimizers.Nadam(learning_rate=0.01)\n",
    "loss_fn = keras.losses.mean_squared_error\n",
    "mean_loss = keras.metrics.Mean()\n",
    "metrics = [keras.metrics.MeanAbsoluteError()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9976e557-e451-4cab-85f6-b6f1cec99629",
   "metadata": {},
   "source": [
    "- We create two nested loops: one for the epochs, the other for the batches within an epoch.\n",
    "- Then we sample a random batch from the training set.\n",
    "- Inside the tf.GradientTape() block, we make a prediction for one batch (using the model as a function), and we compute the loss: it is equal to the main loss plus the other losses (in this model, there is one regularization loss per layer). Since the mean_squared_error() function returns one loss per instance, we compute the mean over the batch using tf.reduce_mean() (if you wanted to apply different weights to each instance, this is where you would do it). The regularization losses are already reduced to a single scalar each, so we just need to sum them (using tf.add_n() , which sums multiple tensors of the same shape and data type).\n",
    "- Next, we ask the tape to compute the gradient of the loss with regard to each trainable variable (not all variables!), and we apply them to the optimizer to perform a Gradient Descent step.\n",
    "- Then we update the mean loss and the metrics (over the current epoch), and we display the status bar.\n",
    "- At the end of each epoch, we display the status bar again to make it look complete and to print a line feed, and we reset the states of the mean loss and the metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6c7622c1-1c54-40f1-8e98-e7310b940dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-14 18:51:22.859249: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13003/13003 - mean: 1.2108 - mean_absolute_error: 0.5826\n",
      "Epoch 2/5\n",
      "13003/13003 - mean: 0.6803 - mean_absolute_error: 0.5441\n",
      "Epoch 3/5\n",
      "13003/13003 - mean: 0.7305 - mean_absolute_error: 0.5437\n",
      "Epoch 4/5\n",
      "13003/13003 - mean: 0.6671 - mean_absolute_error: 0.5387\n",
      "Epoch 5/5\n",
      "13003/13003 - mean: 0.6715 - mean_absolute_error: 0.5449\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, n_epochs + 1):\n",
    "    print(\"Epoch {}/{}\".format(epoch, n_epochs))\n",
    "    for step in range(1, n_steps + 1):\n",
    "        X_batch, y_batch = random_batch(X_train_scaled, y_train)\n",
    "        # examize the gradient batch \n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(X_batch)\n",
    "            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "            loss = tf.add_n([main_loss] + model.losses)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        for variable in model.variables:\n",
    "            if variable.constraint is not None:\n",
    "                variable.assign(variable.constraint(variable))\n",
    "        mean_loss(loss)\n",
    "        for metric in metrics:\n",
    "            metric(y_batch, y_pred)\n",
    "        print_status_bar(step * batch_size, len(y_train), mean_loss, metrics)\n",
    "    print_status_bar(len(y_train), len(y_train), mean_loss, metrics)\n",
    "    for metric in [mean_loss] + metrics:\n",
    "        metric.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495cf95b-1317-4a9b-aa4a-bfdd6e3e6c31",
   "metadata": {},
   "source": [
    "# Custom Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aaa17cc7-eb14-4732-a85c-1d6290af9044",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMomentumOptimizer(keras.optimizers.Optimizer):\n",
    "    def __init__(self, learning_rate=0.001, momentum=0.9, name=\"MyMomentumOptimizer\", **kwargs):\n",
    "        \"\"\"Call super().__init__() and use _set_hyper() to store hyperparameters\"\"\"\n",
    "        super().__init__(name, **kwargs)\n",
    "        self._set_hyper(\"learning_rate\", kwargs.get(\"lr\", learning_rate)) # handle lr=learning_rate\n",
    "        self._set_hyper(\"decay\", self._initial_decay) # \n",
    "        self._set_hyper(\"momentum\", momentum)\n",
    "    \n",
    "    def _create_slots(self, var_list):\n",
    "        \"\"\"For each model variable, create the optimizer variable associated with it.\n",
    "        TensorFlow calls these optimizer variables \"slots\".\n",
    "        For momentum optimization, we need one momentum slot per model variable.\n",
    "        \"\"\"\n",
    "        for var in var_list:\n",
    "            self.add_slot(var, \"momentum\")\n",
    "\n",
    "    @tf.function # TF function\n",
    "    def _resource_apply_dense(self, grad, var):\n",
    "        \"\"\"Update the slots and perform one optimization step for one model variable\n",
    "        \"\"\"\n",
    "        var_dtype = var.dtype.base_dtype\n",
    "        lr_t = self._decayed_lr(var_dtype) # handle learning rate decay\n",
    "        momentum_var = self.get_slot(var, \"momentum\")\n",
    "        momentum_hyper = self._get_hyper(\"momentum\", var_dtype)\n",
    "        momentum_var.assign(momentum_var * momentum_hyper - (1. - momentum_hyper)* grad)\n",
    "        var.assign_add(momentum_var * lr_t)\n",
    "\n",
    "    def _resource_apply_sparse(self, grad, var):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {\n",
    "            **base_config,\n",
    "            \"learning_rate\": self._serialize_hyperparameter(\"learning_rate\"),\n",
    "            \"decay\": self._serialize_hyperparameter(\"decay\"),\n",
    "            \"momentum\": self._serialize_hyperparameter(\"momentum\"),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "da5631ca-1dfc-465f-94b5-0fd1352799b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "407/407 [==============================] - 0s 677us/step - loss: 3.6103\n",
      "Epoch 2/5\n",
      "407/407 [==============================] - 0s 671us/step - loss: 1.3140\n",
      "Epoch 3/5\n",
      "407/407 [==============================] - 0s 673us/step - loss: 0.8355\n",
      "Epoch 4/5\n",
      "407/407 [==============================] - 0s 665us/step - loss: 0.7174\n",
      "Epoch 5/5\n",
      "407/407 [==============================] - 0s 664us/step - loss: 0.6776\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1b1015bb50>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model = keras.models.Sequential([keras.layers.Dense(1, input_shape=[8])])\n",
    "model.compile(loss=\"mse\", optimizer=MyMomentumOptimizer())\n",
    "model.fit(X_train_scaled, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0361fd-7ea5-4c29-baa2-83b618800de3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
